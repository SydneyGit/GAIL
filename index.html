<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My GitHub Pages Images</title>
    <style>
        iframe {
        border: none;
        }
        /* --- NEW CSS FOR TABLE STYLING --- */
        table {
            /*width: 80%; /* Make table take 80% of its container's width */
            font-size: 1.2rem;
            border-collapse: collapse; /* Collapses borders between cells */
            margin: 2rem auto; /* Center the table and add top/bottom margin */
            font-family: sans-serif; /* Set a clean font for the table */
            margin-left: 10rem;   /*  */
            margin-right: 10rem; /*  */
        }

        th, td {
            border: 1px solid #ddd; /* Light gray border for cells */
            padding: 0.8rem;      /* Space inside cells */
            text-align: left;     /* Align text to the left */
        }

        th {
            background-color: #f2f2f2; /* Light gray background for headers */
            color: #333;             /* Darker text for headers */
            font-weight: bold;       /* Ensure headers are bold */
            text-align: center;      /* Center header text */
        }

        tr:nth-child(even) { /* Stripe effect: style every second row */
            background-color: #f9f9f9;
        }

        /* Optional: Style the first row specifically if it's not in thead */
        /* tr:first-child {
            background-color: #f2f2f2;
            font-weight: bold;
            text-align: center;
        } */


        /* --- Base Font Size Setup (for REM units) --- */
        /* It's good practice to set a base font-size on the html element. */
        /* 16px is the browser's default. You can change it if you want your
           rem units to be based on a different default.
           e.g., 1rem = 16px. So 2rem = 32px. */
        html {
            font-size: 16px; /* Base font size for 'rem' units */
        }
        /* --- Global Styles --- */
        body {
            font-family: sans-serif;
            text-align: left;
            margin-top: 50px;
            color: #333;
            margin-top: 0rem;   /* Space above all headings */
            margin-bottom: 0rem; /* Space below all headings */
            /*margin: 0 2rem; /* 0 top/bottom, 2rem left/right */
            margin-left: 10rem;  /* Example: 2 times the root font size */
            margin-right: 10rem; /* Example: 2 times the root font size */
            
        }
        /* --- Heading Styles (using REM) --- */
        h1, h2, h3, h4, h5, h6 {
            font-family: 'Montserrat', sans-serif; /* Your chosen heading font */
            margin-top: 3rem;   /* Space above all headings */
            margin-bottom: 3rem; /* Space below all headings */
            margin-left: 0rem;  /* Example: 2 times the root font size */
            margin-right: 0rem; /* Example: 2 times the root font size */
            /* Headings are block elements, so they naturally take full width. */
            /* By default, they have some browser-defined margins, which you are now overriding. */
        }

        h1 {
            font-size: 5rem; /* 3 times the base html font size (e.g., 3 * 16px = 48px) */
            text-align: center;
            /*font-family: "Times New Roman", Times, serif; /* Example: A common serif font for headings */
            color: #333;
        }
        h2 {
            font-size: 3rem; /* 2 times the base html font size (e.g., 2 * 16px = 32px) */
            text-align: left;
            color: #333;
        }
        h3 {
            font-size: 1.5rem; /* 1.5 times the base html font size (e.g., 2 * 16px = 32px) */
            text-align: left;
            color: #555;
        }
        p {
            /*font-family: Verdana, Geneva, sans-serif; /* Another sans-serif option */
            font-size: 1.1rem; /* 1.1 times the base html font size (e.g., 1.1 * 16px = 17.6px) */
            line-height: 1.6; /* Spacing between lines for readability */
            text-align: left;
            /*max-width: 800px; /* Limit paragraph width for readability */
            /*margin: 0 auto 15px auto; /* Center paragraph and add bottom space */
            margin-left: 0rem;  /* Example: 2 times the root font size */
            margin-right: 0rem; /* Example: 2 times the root font size */
        }

        /* --- NEW TEXT BODY FORMAT 1: Introduction Text --- */
        .intro-text {
            font-size: 1.3rem; /* Larger font size */
            font-weight: bold; /* Make it bold */
            /*color: #0056b3;   /* A different color */
            /*max-width: 700px; /* Maybe a slightly narrower width */
            text-align: justify; /* Center the text */
            /*margin-bottom: 2rem; /* More space below */
        }

        .info-text {
            font-size: 1.1rem; /* Larger font size */
            /*font-weight: bold; /* Make it bold */
            /*color: #0056b3;   /* A different color */
            /*max-width: 700px; /* Maybe a slightly narrower width */
            text-align: start; /* Center the text */
            line-height: 1.6; /* Spacing between lines for readability */
            margin-left: 0rem;  /* Example: 2 times the root font size */
            margin-right: 0rem; /* Example: 2 times the root font size */
        }

        .image-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 20px; /* Space between images */
            margin-top: 30px;
            /*align-items: center;          /* Center-align each image horizontally */
        }
        .image-container img {
            max-width: 600px; /* Limit image size for display */
            height: auto;
            border: 1px solid #ddd;
            box-shadow: 2px 2px 5px rgba(0,0,0,0.2);
        }

        /*.image-wrapper {
       /* max-width: 1000px;     /* Adjust based on your desired row width */
        /*flex: 1 1 400px;      /* Responsive flexibility */
    /*}


        /* Style to remove the border from images in the second container */
        .image-container-no-border {
            /*max-width: 800px; /* Limit image size for display */
            /*height: auto;
            /*border: 1px solid #ddd;
            box-shadow: 2px 2px 5px rgba(0,0,0,0.2);
            padding: 10px; /* Optional padding */
            /*margin: 10px; /* Optional margin */
            text-align: center; /* Center the image */
        }

        .image-container-no-border img {
            border: none; /* Removes any border */
            max-width: 90%;
            height: auto;
        }
    </style>
</head>
<body>
    <div class="image-container-no-border">
        <img src="UoE.png" alt="UoE">

    </div>
    <h1>GAIL: GenAI for technical design</h1>



    <div class="image-container">
        <img src="GAIL_graphic1.png" alt="intro graphic">

    </div>


    <h3>Envision video</h3>
    <a href="https://www.youtube.com/watch?v=OZPzcniXgkI" target="_blank">Link to the video</a>

    
    <p class="intro-text">To be held at University of Edinburgh Sep 2, 2025</p>
    <p class="intro-text">
       We are inviting you to participate in a cross-disciplinary workshop to help shape the Edinburgh Roadmap for Generative Technical Design. 
       <br>This initiative brings together researchers from Engineering, Informatics, ECA, and ISSTI to explore the future of Generative AI 
       in technical design domains, including engineering, architecture, product design and simulation. 
       <br>The aim of the workshop is to:
    
       <br>Â· &nbsp;&nbsp;&nbsp;&nbsp; Identify key opportunities and challenges for the use of GenAI in technical design.
        
       <br>Â· &nbsp;&nbsp;&nbsp;&nbsp; Define knowledge gaps and critical research questions.
        
       <br>Â· &nbsp;&nbsp;&nbsp;&nbsp; Lay the groundwork for exemplar datasets, use-cases, and benchmarking approaches.
        
       <br>We are keen to involve users, commercial developers and researchers with interests in AI, design computation, simulation, manufacturing, 
       digital geometry, human-computer interaction, and related fields.
        
       <br>The day will include presentations giving overviews of various AI technologies and applications in technical design and discussion of 
       draft roadmaps and assessment benchmarks for technical design applications.


    <h2>Workshop agenda</h2>
    <table>
        <thead>
            <tr>
                <th>Time</th>
                <th>Activity</th>
                <th>Speakers</th>
            </tr>
        </thead>
        <tbody>
            
            <tr>
                <td>Talk 1</td>
                <td>Allin Groom</td>
                <td>Principal Research Scientist, Industry Futures, Autodesk Research</td>
                <td><a href="2025_ AI_in_AEC_Autodesk Research.pdf" target="_blank">Link to the talk</a></td>
            </tr>
            <tr>
                <td>Talk 2</td>
                <td>Andrew Sherlock</td>
                <td>Director of Data driven Manufacturing, National Manufacturing Institute of Scotland (NMIS)</td>
                <td>Link to the talk</td>
            </tr>
            <tr>
                <td>Talk 3</td>
                <td>Richard Coyne</td>
                <td>Professor of Architectural Computing, University of Edinburgh</td>
                <td><a href="Richard Coyne GenAI.pdf" target="_blank">Link to the talk</a></td>
            </tr>
            <tr>
                <td>Talk 4</td>
                <td>Tim Drysdale</td>
                <td>Chair in Technology Enhanced Science Education, University of Edinburgh</td>
                <td><a href="Drysdale-AI-Design-Sept-2025.pdf" target="_blank">Link to the talk</a></td>
            </tr>
            <tr>
                <td>Talk 5</td>
                <td>Vaishak Belle</td>
                <td>Reader in Logic and Learning, University of Edinburgh</td>
                <td><a href="Vaishak Belle genAI-workshoptalk.pdf" target="_blank">Link to the talk</a></td>
            </tr>
            <tr>
                <td>Talk 6</td>
                
                <td>Jonathan Corney</td>
                <td>Chair of Digital Manufacturing, University of Edinburgh</td>
                <td><a href="Personal PerspectiveJC.pdf" target="_blank">Link to the talk</a></td>
            </tr>
            <tr>
                <td>Talk 7</td>
                <td>Shuang (Sydney) Li</td>
                <td>Research Associate in Digital Manufacturing, University of Edinburgh</td>
                <td><a href="GAIL_workshop.pdf" target="_blank">Link to the talk</a></td>
            </tr>
        </tbody>
    </table> 

    <h3>Agenda PDF</h3>
    <ul>
    <li><a href="Workshop Agendav2.pdf" target="_blank">Link to the agenda</a></li>
    </ul>

    <h3>Workshop registration</h3>
    <a href="https://forms.office.com/Pages/ResponsePage.aspx?id=sAafLmkWiUWHiRCgaTTcYWpaL-8rzKpFsbBIb7DHxwZUQzg1UElNSFZZV05QM0c4VE5NUkcyRlVMWi4u&origin=Invitation&channel=0" target="_blank">Registration here!</a>
    <p>Any questions please contact us! ðŸ‘‡ If you require any resource, also contact us! ðŸ‘‡</p>
    <a href="mailto:genai.4.technical.design@gmail.com">Email us</a>


    <h2>Projet team</h2>
    <p>Jonathan Corney, Digital Design & Manufacture, School of Engineering

        <br>Shuang Li, AI Applications Researcher, School of Engineering
        
        <br>Miguel Paredes Maldonado, Architectural Design, Edinburgh College of Art
        
        <br>Amir Vaxman, Geometry Processing Group, School of Informatics
        
        <br>Matjaz Vidmar, Innovation & development (eco)systems, Institute for Study of Science, Technology & Innovation 
    </p>


    <h2>What to expect</h2>
    <h3>Roadmap of GenAI impacts on technical deisgn</h3>
    <div class="image-container">
        <img src="Roadmap.png" alt="Roadmap">

    </div>
    <h3>Benchmark datasets and cutting-edge techniques</h3>


    <h2>Datasets for technical design</h2>
    <h3>Mechanical component dataset</h3>
    <p class="info-text">This a large-scale annotated mechanical components benchmark for classification and retrieval tasks named 
        Mechanical Components Benchmark (MCB): a large-scale dataset of 3D objects of mechanical components.
        The dataset enables data-driven feature learning for mechanical components. 
        Exploring the shape descriptor for mechanical components is essential to computer vision and manufacturing applications. 
        This CAD dataset consists of 58,696 models and their annotations.</p>
    <a href="https://engineering.purdue.edu/cdesign/wp/a-large-scale-annotated-mechanical-components-benchmark-for-classification-and-retrieval-tasks-with-deep-neural-networks/">Link to the dataset</a>
    <div class="image-container">
        <img src="MechanicalComponentDataset.png" alt="MCB">

    </div>


    <h3>Manufacturing method dataset</h3>
    <p class="info-text">This dataset contains examples of mechanical parts created by five different manufacturing methods: 
        Fabricated, Forged, Other, Sheet Metal, and Turned. 
        The manufacturing method associated with each component, refers to the main process used to produce the specific part, 
        rather than an exhaustive description of its production process.
        The number of parts for each manufacturing method is about 8,600 and the total number of parts is 43,380. 
    </p>
    <a href="https://www.researchgate.net/publication/390695850_Automatic_Recognition_of_a_Component's_Manufacturing_Method">Link to the dataset</a>
    <div class="image-container">
        <img src="ManufacturingMethodDataset.png" alt="Jason's dataset">

    </div>


    <h3>CAD modelling sequence dataset</h3>
    <p class="info-text">Deep generative models of 3D shapes have received a
        great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point
        clouds, and polygon meshes. This the first 3D generative model for a drastically different shape representationâ€”
        describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds,
        CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design
        tasks. This CAD dataset consists of 178,238 models and their CAD construction sequences.</p>
    <a href="https://github.com/ChrisWu1997/DeepCAD">Link to the dataset</a>
    <div class="image-container">
        <img src="CADsequence1.png" alt="Jason's dataset">
    </div>
    <div class="image-container">
        <img src="CADsequence2.png" alt="Jason's dataset">
    </div>



    <h3>Feature recognition dataset</h3>
    <p class="info-text">This dataset is designed for research and development in machining feature recognition using annotated CAD models. 
        Machining feature recognition is a critical task in computer-aided manufacturing, 
        where the goal is to automatically identify features such as holes, pockets, 
        and slots from CAD geometryâ€”an essential step for process planning and automation.
        This dataset consists of 2,100 CAD models, their multi-view images and machining feature annotations. 
    </p>
    <div class="image-container">
        <img src="FeatureRecognitionDataset.png" alt="FeatureRecognitionDataset">

    </div>



    <h3>Change detection dataset</h3>
    <p class="info-text">Although mechanical CAD is a mature technology, assessing the functional and 
        manufacturing impact of design changes remains a labor-intensive manual task. To address this issue, 
        the concept of "3D CAD Change Detection and Classification" (3D-CDC) is introduced 
        and a novel dataset, "3D-GCD" (3D Geometric Change Dataset) is present, 
        consisting of pairs of 3D CAD components with a variety of technically plausible changes. 
        The dataset represents 3D components in multiple 2D image formats (e.g., grayscale and comparative heatmaps) 
        with renderings generated from single or multiple viewpoints.
        This dataset consists of 5,577 pairs of 3D CAD models (each pair has a nominal part and a family part), and their view images.
    </p>
    <div class="image-container">
        <img src="ChangeDetectionDataset1.png" alt="ChangeDetectionDataset1">

    </div>
    <div class="image-container">
        <img src="ChangeDetectionDataset2.png" alt="ChangeDetectionDataset2">

    </div>



    <h2>Techniques & products for generative technical deisgn</h2>
    <h3>RAG</h3>
    <p class="info-text">Engineering design and manufacture are inherently multimodal activities in which engineers consult and produce diverse data 
    and representations across various engineering disciplines (e.g., solid mechanics, fluid dynamics) and product lifecycle stages 
    (e.g., manufacture, assembly planning, maintenance). Although well-established digital formats exist for these representations, 
    their use remains restricted within specialist applications, creating silos that limit cross-domain integration. 
    Engineers must therefore routinely act as integrators, manually extracting and interpreting key data values to perform comparative 
    or qualitative reasoning tasks crucial to planning, design, and manufacturing processes. This MechRAG 
    (Mechanical Retrieval-Augmented Generation), a multimodal large language model architecture is designed to unify information 
    from multiple engineering representations typically found in Computer-Aided Engineering (CAE) and Computer-Aided Design (CAD) environments. 
    We propose a three-level taxonomy of MechRAG use-cases and systematically investigate the performance implications of five critical configuration variables 
    in our prototype implementation. Results demonstrate that MechRAG achieves high accuracy in routinely performed mechanical activities 
    including data-management and classification tasks, and effectively replicates engineer-level reasoning in more inferential and subjective contexts. 
    Our findings suggest that such conversational interfaces could significantly enhance engineering productivity, facilitate new interaction paradigms, 
    and drive transformative workflows across various stages of design and manufacturing.</p>
    <a href="MITPresentationv2.pdf" target="_blank">View Slides present at DESAI25, MIT</a>

    <div class="image-container">
        <img src="RAG4MechanicalEngineering.png" alt="RAG for CAD">

    </div>

    <h3>Fine-tuning</h3>
    <p class="info-text">This multi-modal LLM aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD
        models based on the userâ€™s inputs in the form of textual description, images, 2D drawings, or even a combination of them. 
        This system is capable of generating parametric CAD models conditioned on the multimodal input.
        Specifically, for the fine-tuning of MLLM, the CAD modelling sequence data is used for the major component in the texual description of CAD models.
        This multi-modal LLM aligns the feature space across these diverse multi-modalities data and CAD modelsâ€™ vectorized
        representations. </p>
    <div class="image-container">
        <img src="GenAI4CAD.png" alt="Fine-tuning for generative CAD">
    </div>

    <p>Stay tuned for more updates! </p>

    <!-- Add these lines at the very end of your body content, just before </body> -->
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>

    
</body>
</html>